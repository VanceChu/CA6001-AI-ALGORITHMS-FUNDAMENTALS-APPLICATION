{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Reinforcement Learning Tutorial & Practice Exercises\n",
        "Tutorial Overview:\n",
        "Before attempting the exercises, let's review key reinforcement learning concepts:\n",
        "\n",
        "Reinforcement learning involves an agent learning to make decisions by interacting with an environment\n",
        "The agent receives rewards or penalties based on its actions\n",
        "The goal is to learn a policy that maximizes expected cumulative reward\n",
        "Key components include:\n",
        "\n",
        "<br>State (S): The current situation\n",
        "<br>Action (A): What the agent can do\n",
        "<br>Reward (R): Immediate feedback\n",
        "<br>Policy (π): Strategy for choosing actions\n",
        "<br>Value function (V): Expected future reward from a state\n",
        "<br>Q-function (Q): Expected future reward from a state-action pair\n",
        "\n",
        "Practice Exercises\n",
        "<br> Question 1: Policy Evaluation\n",
        "Consider a simple grid world where a robot needs to reach a goal. The robot can move Up, Down, Left, or Right. Each move has a -1 reward, reaching the goal\n",
        "gives +10, and hitting a wall keeps the robot in place with -1 reward."
      ],
      "metadata": {
        "id": "R5kdv7-93u6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "S . . .\n",
        ". # . .\n",
        ". . . G"
      ],
      "metadata": {
        "id": "lw-e39Mt35eu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "S = Start, G = Goal, # = Wall, . = Empty cell\n",
        "<br> A) Write out the value function V(s) for each state after one iteration of policy evaluation, assuming:\n",
        "\n",
        "Discount factor γ = 0.9\n",
        "The robot follows a random policy (equal probability of each action)\n",
        "Initial V(s) = 0 for all states except the goal, where V(G) = 10\n",
        "\n",
        "B) Explain why certain states have the values you calculated."
      ],
      "metadata": {
        "id": "V6rc96O24HOm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***your answer here***"
      ],
      "metadata": {
        "id": "JvAhDa5zf2b8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2: Q-Learning Application\n",
        "You're training an agent to play a game where it can choose between two actions: \"Safe\" and \"Risky\".\n",
        "\n",
        "\"Safe\" always gives a reward of +2.\n",
        "\"Risky\" gives either +5 or -1 with equal probability.\n",
        "\n",
        "A) If γ = 0.8, calculate the Q-value for the \"Safe\" action after observing these three sequences:\n",
        "\n",
        "<br>Safe → +2 → Safe → +2\n",
        "<br>Safe → +2 → Risky → +5\n",
        "<br>Safe → +2 → Risky → -1\n",
        "<br> hint: multiply γ with reward for 2nd step\n",
        "\n",
        "B) Based on these Q-values, what action would an ε-greedy policy choose with ε = 0.2?\n"
      ],
      "metadata": {
        "id": "IMcUORQ072vo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***your answer here***\n"
      ],
      "metadata": {
        "id": "nAzdXXpk8WwY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LkSJtO7C8Z4T"
      }
    }
  ]
}