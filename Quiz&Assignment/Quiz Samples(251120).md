### Question 1

**Which of the following methods is typically used for balancing exploration and exploitation?**

- A. Gradient Descent
- B. Epsilon Greedy Strategy
- C. Back propagation
- D. Feature Selection

---

### Question 2

**A robot is trained using RL to navigate a maze. If it receives a +10 reward for reaching the goal and a -1 reward for each step taken, what type of behavior is the robot likely to develop?**

- A. It will take the longest possible route to explore the environment
- B. It will learn the shortest path to the goal
- C. It will avoid reaching the goal to keep exploring
- D. It will take random actions indefinitely

---

### Question 3

**Which of the following is NOT a key component of an RL system?**

- A. Environment
- B. Reward Signal
- C. Labeled Data
- D. Cost Function

---

### Question 4

**Which RL algorithm estimates the value of state–action pairs and uses them to make decisions?**

- A. Policy Gradient
- B. Q Learning
- C. Evolution Strategies
- D. Principal Component Analysis

---

### Question 5

**In RL, what does an “action” represent?**

- A. The immediate reward received by the agent
- B. A choice made by the agent that affects the environment
- C. The function mapping states to values
- D. The final outcome of an episode

---

### Question 6

**What is the key difference between on-policy and off-policy learning?**

- A. Off-policy methods use past experiences, while on-policy methods only use current experiences
- B. On-policy methods always perform better than off-policy methods
- C. Off-policy methods do not require exploration
- D. On-policy methods use a separate network for learning